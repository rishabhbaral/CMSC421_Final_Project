# -*- coding: utf-8 -*-
"""CMSC421.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12jlKGn9m17CsGTWIUlKkZjj1p4UD_FCN

Sub-Task 1.1 Curriculum Tranchmark
"""


'''
  have data
  tasks
  and eval metrics
  compare performance of different models

  what do we need now
'''

from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline
from transformers import TrainingArguments, Trainer, pipeline
from datasets import load_dataset

dataset = load_dataset('glue', 'cola')

model_name = "QCRI/bert-base-multilingual-cased-pos-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)

pipeline = TokenClassificationPipeline(model=model, tokenizer=tokenizer)
outputs = pipeline("A test example")

from datasets import load_dataset
dataset = load_dataset("glue", "cola")
dataset = dataset.map(lambda examples: {"input_ids": examples['idx']}, batched=True)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
def preprocess_function(examples):
    return tokenizer(examples["sentence"], padding=True, truncation=True, max_length=512, add_special_tokens = True)

tokenized_dataset = dataset.map(preprocess_function, batched=True)
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir='./results',
    learning_rate=2e-5,
    per_device_train_batch_size= 16,
    per_device_eval_batch_size= 16,
    num_train_epochs=5,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

eval_results = trainer.evaluate(tokenized_dataset['validation'])
print(eval_results)



from datasets import load_dataset

dataset = load_dataset('glue', 'cola')

print(dataset['train'][2000])
